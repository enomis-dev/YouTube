{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354c2fc2",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "In this notebook you will learn the basis of reinforcement learning in python\n",
    "\n",
    "Sections:\n",
    "- 1) Example 1\n",
    "- 2) Example 2\n",
    "- 3) Training and evaluate a model\n",
    "- 4) Environment vectorization\n",
    "- 5) Build a custom model\n",
    "- 6) Simple rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe638cb5",
   "metadata": {},
   "source": [
    "# 1) Example 1\n",
    "\n",
    "This is a first example to illustrate the concept of reinforcement learning and to play with an existing environment.\n",
    "The lunar lander environment recreate the environment of a lander arriving on the moon and subject to the gravity force, the objectve is to land in the delimited area in a perfect way by using the minor number of corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824cb17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes: 5\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "#  gym library is used for developing and comparing reinforcement learning algorithms\n",
    "\n",
    "# This line creates an environment for the \"LunarLander-v2\" game. \n",
    "# The render_mode=\"human\" parameter is used to render the environment in a way that is viewable by humans.\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "# This sets a seed for the environment's action space to ensure reproducibility. \n",
    "# Seeding makes the random actions predictable and repeatable.\n",
    "env.action_space.seed(42)\n",
    "\n",
    "# This resets the environment to its initial state and returns the initial observation and some additional info. \n",
    "# The seed=42 ensures that the reset is reproducible.\n",
    "observation, info = env.reset(seed=42)\n",
    "episode_count = 0 # count number of episodes i.e. number of simulations\n",
    "\n",
    "# Loop runs for 500 iterations, performing the following actions in each iteration\n",
    "for _ in range(500):\n",
    "    # env.action_space.sample(): Randomly samples an action from the action space.\n",
    "    # env.step(action): Applies the sampled action to the environment and returns the following:\n",
    "    # observation: The new state of the environment after the action is taken.\n",
    "    # reward: The reward received after taking the action.\n",
    "    # terminated: A boolean indicating if the episode has ended (e.g., the lander has crashed or landed successfully).\n",
    "    # truncated: A boolean indicating if the episode was truncated (e.g., due to a time limit).\n",
    "    # info: Additional diagnostic information.\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    # If the episode has ended (terminated or truncated is True), the environment is reset to the initial state.\n",
    "    if terminated or truncated:\n",
    "        episode_count += 1\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Closes the environment and performs any necessary cleanup.\n",
    "env.close()\n",
    "print(f\"Total episodes: {episode_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d81c03",
   "metadata": {},
   "source": [
    "# 2) Example 2\n",
    "\n",
    "This is a second example to illustrate the concept of reinforcement learning and to play with an existing environment.\n",
    "The objective of this environment is to provide a simple but yet interesting envronment to test modes for autonomous driving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbdd26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes: 0\n"
     ]
    }
   ],
   "source": [
    "# This line creates an environment for the \"CarRacing-v2\" game. \n",
    "# The render_mode=\"human\" parameter is used to render the environment in a way that is viewable by humans.\n",
    "env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
    "\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "episode_count = 0\n",
    "\n",
    "for _ in range(500):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    # If the episode has ended (terminated or truncated is True), the environment is reset to the initial state.\n",
    "    if terminated or truncated:\n",
    "        episode_count += 1\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Closes the environment and performs any necessary cleanup.\n",
    "env.close()\n",
    "print(f\"Total episodes: {episode_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bb433",
   "metadata": {},
   "source": [
    "# 3) Training a model and evaluate it\n",
    "\n",
    "Several strategies can be chosen to trainour model. In this example we will use \"Machine learning policy\".\n",
    "Full view on the implemented methods and their applicability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31830c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 85.5     |\n",
      "|    ep_rew_mean      | -460     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 342      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.42     |\n",
      "|    n_updates        | 60       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.1     |\n",
      "|    ep_rew_mean      | -358     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 641      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.76     |\n",
      "|    n_updates        | 135      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 73.1     |\n",
      "|    ep_rew_mean      | -270     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 877      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.33     |\n",
      "|    n_updates        | 194      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\degas\\Documents\\Github\\YouTube\\Reinforcement_learning\\rl\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -149.4235740008531 +/- 18.611171676267748\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "# Create the model: DQN\n",
    "model = DQN('MlpPolicy', env, seed=42, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000)  # Adjust the number of timesteps as needed\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dqn_lunar_lander\")\n",
    "\n",
    "# Load a trained model\n",
    "model = DQN.load(\"dqn_lunar_lander\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0eb69",
   "metadata": {},
   "source": [
    "# Manual evaluation\n",
    "\n",
    "In the previous section we have evaluated the model thanks to the evaluate_policy method but if we want we can reproduce the same with the following for loop. This is a good excercise just to understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57ef163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -156.4939233016318\n",
      "Episode 2: Total Reward = -133.3727717892838\n",
      "Episode 3: Total Reward = -162.60920327789626\n",
      "Episode 4: Total Reward = -132.82254172894105\n",
      "Episode 5: Total Reward = -129.79511089490825\n",
      "Episode 6: Total Reward = -124.94995358858012\n",
      "Episode 7: Total Reward = -133.52126040626428\n",
      "Episode 8: Total Reward = -141.01516062459905\n",
      "Episode 9: Total Reward = -127.47238629863033\n",
      "Episode 10: Total Reward = -113.97364011031893\n",
      "Episode rewards: [-156.4939233016318, -133.3727717892838, -162.60920327789626, -132.82254172894105, -129.79511089490825, -124.94995358858012, -133.52126040626428, -141.01516062459905, -127.47238629863033, -113.97364011031893]\n",
      "Mean reward over 10 episodes: -135.6025952021054\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "# Load the saved model\n",
    "model = DQN.load(\"dqn_lunar_lander\")\n",
    "\n",
    "# Number of episodes to run\n",
    "num_episodes = 10\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        action, _states = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Render the environment to visualize the agent's performance\n",
    "        env.render()\n",
    "        \n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    # Append the total reward for this episode to the list\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Print the rewards for all episodes\n",
    "print(\"Episode rewards:\", episode_rewards)\n",
    "print(f\"Mean reward over {num_episodes} episodes: {sum(episode_rewards) / num_episodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6405c1",
   "metadata": {},
   "source": [
    "# 4) Parallel training\n",
    "\n",
    "Vectorized environments are environments that run multiple independent copies of the same environment in parallel using multiprocessing. Vectorized environments take as input a batch of actions, and return a batch of observations. This is particularly useful, for example, when the policy is defined as a neural network that operates over a batch of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f023d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3070 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1681        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009970572 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.0119      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 345         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00755    |\n",
      "|    value_loss           | 1.09e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1355        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010859178 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 699         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1281       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01029673 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.33      |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 71.5       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    value_loss           | 294        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1193        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013016429 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 67.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1140        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012237452 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.6        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1089       |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 52         |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01098364 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.66       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.2       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 97.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1034       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 63         |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01236068 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 78.1       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    value_loss           | 103        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 959         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008954002 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 76.2        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 910         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008456614 |\n",
      "|    clip_fraction        | 0.0735      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00928    |\n",
      "|    value_loss           | 65          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006266447 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.84        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    value_loss           | 58.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009970524 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.8         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00906    |\n",
      "|    value_loss           | 46.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 787         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008156115 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.87        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00779    |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the LunarLander-v2 environment\n",
    "def make_env():\n",
    "    return gym.make('LunarLander-v2')\n",
    "\n",
    "# Create a vectorized environment\n",
    "num_envs = 4\n",
    "# DummyVecEnv is used to create a vectorized environment\n",
    "env = DummyVecEnv([make_env for _ in range(num_envs)])\n",
    "\n",
    "# VecFrameStack stacks frames to provide the agent with a sequence of observations, \n",
    "# which can be useful for environments where the current state depends on previous states.\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_lunarlander_parallel\")\n",
    "\n",
    "# To load and use the model\n",
    "model = PPO.load(\"ppo_lunarlander_parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f147a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = [132.08537]\n",
      "Episode 2: Total Reward = [121.57703]\n",
      "Episode 3: Total Reward = [144.12541]\n",
      "Episode 4: Total Reward = [156.30675]\n",
      "Episode 5: Total Reward = [93.6617]\n",
      "Episode 6: Total Reward = [99.62338]\n",
      "Episode 7: Total Reward = [148.38686]\n",
      "Episode 8: Total Reward = [162.59474]\n",
      "Episode 9: Total Reward = [143.88536]\n",
      "Episode 10: Total Reward = [132.70992]\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorized environment\n",
    "# Create a single instance of the LunarLander-v2 environment with frame stacking.\n",
    "num_envs = 1\n",
    "env = DummyVecEnv([make_env for _ in range(num_envs)])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Evaluate the model\n",
    "# Run the model for a specified number of episodes (in this case, 10) and print the total reward for each episode.\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56632688",
   "metadata": {},
   "source": [
    "# 5) Custom basic model\n",
    "\n",
    "In this section we will create a custom environment and we will see how to train a model to achieve our goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8311a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 367      |\n",
      "|    ep_rew_mean     | 85.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 1543     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 495         |\n",
      "|    ep_rew_mean          | -220        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1103        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012386147 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | -0.000219   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 99          |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00919    |\n",
      "|    value_loss           | 173         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 360         |\n",
      "|    ep_rew_mean          | -198        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014215872 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | -0.00194    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 311          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041396124 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | -1.2e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 259          |\n",
      "|    ep_rew_mean          | -160         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070713693 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | -7.51e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 244          |\n",
      "|    ep_rew_mean          | -147         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061284574 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.506       |\n",
      "|    explained_variance   | -3.81e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.9         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000832    |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 226          |\n",
      "|    ep_rew_mean          | -141         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 787          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001646399 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | -2.5e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.5         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.000377     |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 214          |\n",
      "|    ep_rew_mean          | -136         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 762          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001711522 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.516       |\n",
      "|    explained_variance   | -1.19e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -6.45e-05    |\n",
      "|    value_loss           | 66.5         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 209        |\n",
      "|    ep_rew_mean          | -131       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00847102 |\n",
      "|    clip_fraction        | 0.0535     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | -1.67e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 30.5       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.000956   |\n",
      "|    value_loss           | 79.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 195          |\n",
      "|    ep_rew_mean          | -130         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 742          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047566756 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | -8.34e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.8         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -2.14e-05    |\n",
      "|    value_loss           | 97           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 164           |\n",
      "|    ep_rew_mean          | -109          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 735           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 30            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013810821 |\n",
      "|    clip_fraction        | 0.0197        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.402        |\n",
      "|    explained_variance   | -7.15e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 47.9          |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -7.43e-05     |\n",
      "|    value_loss           | 87.8          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 154         |\n",
      "|    ep_rew_mean          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 738         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004330529 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.5        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000355   |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 142          |\n",
      "|    ep_rew_mean          | -94.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 740          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023039768 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.322       |\n",
      "|    explained_variance   | -4.77e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.4         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000868    |\n",
      "|    value_loss           | 68.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 140          |\n",
      "|    ep_rew_mean          | -87.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 745          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012581034 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.4         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000482    |\n",
      "|    value_loss           | 82           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 135          |\n",
      "|    ep_rew_mean          | -81.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 753          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002827168 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.4         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 1.96e-05     |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 128          |\n",
      "|    ep_rew_mean          | -78.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 762          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015576689 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.7         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -2.05e-05    |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 126          |\n",
      "|    ep_rew_mean          | -74.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 770          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031188088 |\n",
      "|    clip_fraction        | 0.00605      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.3         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | 0.000168     |\n",
      "|    value_loss           | 76.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -75.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011684664 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.2         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 93           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 122          |\n",
      "|    ep_rew_mean          | -75          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 783          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016353888 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 9.07e-05     |\n",
      "|    value_loss           | 55.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -74.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 787          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014037067 |\n",
      "|    clip_fraction        | 0.00557      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.31        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.7         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | 0.000598     |\n",
      "|    value_loss           | 66.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 113          |\n",
      "|    ep_rew_mean          | -74.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015716945 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.4         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000414    |\n",
      "|    value_loss           | 80           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 114          |\n",
      "|    ep_rew_mean          | -74.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 798          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012505965 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.2         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000311    |\n",
      "|    value_loss           | 65.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 115         |\n",
      "|    ep_rew_mean          | -72.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002103059 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.3        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.000751   |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 117          |\n",
      "|    ep_rew_mean          | -71.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007534177 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 71.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 118          |\n",
      "|    ep_rew_mean          | -69.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006040819 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.5         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | 2.42e-05     |\n",
      "|    value_loss           | 99.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 116           |\n",
      "|    ep_rew_mean          | -70.4         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 813           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 65            |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086909236 |\n",
      "|    clip_fraction        | 0.0143        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.249        |\n",
      "|    explained_variance   | 1.07e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 48.2          |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00101      |\n",
      "|    value_loss           | 86.8          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -70.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013649669 |\n",
      "|    clip_fraction        | 0.00913      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.271       |\n",
      "|    explained_variance   | 0.000988     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.6         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000341    |\n",
      "|    value_loss           | 72.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -71.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003831634 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.00215      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.3         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -3.65e-05    |\n",
      "|    value_loss           | 91.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 122          |\n",
      "|    ep_rew_mean          | -71.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013095235 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.0124       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.7         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 85.6         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 120           |\n",
      "|    ep_rew_mean          | -72           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 823           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0222517e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.244        |\n",
      "|    explained_variance   | 0.0216        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 34.4          |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | 2.37e-05      |\n",
      "|    value_loss           | 93.6          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -73.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.135881e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.4         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -5.39e-05    |\n",
      "|    value_loss           | 92           |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 121           |\n",
      "|    ep_rew_mean          | -72.5         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 827           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 79            |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011971069 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.243        |\n",
      "|    explained_variance   | 0.0326        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 60.7          |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -7.3e-05      |\n",
      "|    value_loss           | 86.8          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 117           |\n",
      "|    ep_rew_mean          | -70.8         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 828           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 81            |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047545566 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.226        |\n",
      "|    explained_variance   | 0.0387        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 47.3          |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.000302     |\n",
      "|    value_loss           | 79.9          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -71          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010334107 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.0386       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.5         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    value_loss           | 77.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 114           |\n",
      "|    ep_rew_mean          | -71.6         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 833           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 85            |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5149773e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.265        |\n",
      "|    explained_variance   | 0.0424        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 42.8          |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -1.11e-05     |\n",
      "|    value_loss           | 82.3          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 115           |\n",
      "|    ep_rew_mean          | -70.7         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 836           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 88            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4475164e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.255        |\n",
      "|    explained_variance   | 0.047         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 61.9          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | 8.42e-05      |\n",
      "|    value_loss           | 78.6          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 115          |\n",
      "|    ep_rew_mean          | -70.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 838          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005304217 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.0483       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.6         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000166    |\n",
      "|    value_loss           | 91.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -70.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 840          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007586697 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.067        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.1         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    value_loss           | 71.5         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 117           |\n",
      "|    ep_rew_mean          | -70.4         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 841           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 94            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.0178154e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.204        |\n",
      "|    explained_variance   | 0.075         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 37.8          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | 1.04e-05      |\n",
      "|    value_loss           | 65.5          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 115          |\n",
      "|    ep_rew_mean          | -70.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 841          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008460237 |\n",
      "|    clip_fraction        | 0.00752      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.215       |\n",
      "|    explained_variance   | 0.0722       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.8         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00018     |\n",
      "|    value_loss           | 94.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -69.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 841          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010620726 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.0859       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.4         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -9.49e-05    |\n",
      "|    value_loss           | 62.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 114          |\n",
      "|    ep_rew_mean          | -70.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 842          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010384828 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.093        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 89.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 114          |\n",
      "|    ep_rew_mean          | -71.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017935276 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.5         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 65.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -72.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 830          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010538434 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.7         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -7.94e-05    |\n",
      "|    value_loss           | 95.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -73.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022243871 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.6         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000701    |\n",
      "|    value_loss           | 79           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 126          |\n",
      "|    ep_rew_mean          | -74.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012491383 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.378       |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.2         |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | 0.000147     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 130           |\n",
      "|    ep_rew_mean          | -75           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 808           |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 119           |\n",
      "|    total_timesteps      | 96256         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013385146 |\n",
      "|    clip_fraction        | 0.0231        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.389        |\n",
      "|    explained_variance   | 0.14          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 54.4          |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | 0.000489      |\n",
      "|    value_loss           | 94            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 134          |\n",
      "|    ep_rew_mean          | -77          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 803          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015742434 |\n",
      "|    clip_fraction        | 0.00605      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 67.3         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000954    |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 136          |\n",
      "|    ep_rew_mean          | -77.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 794          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037376448 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.1         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000254    |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water Level: 102.47\n",
      "Water Level: 102.57\n",
      "Water Level: 102.47\n",
      "Water Level: 102.37\n",
      "Water Level: 102.47\n",
      "Water Level: 102.37\n",
      "Water Level: 102.27\n",
      "Water Level: 102.17\n",
      "Water Level: 102.07\n",
      "Water Level: 101.97\n",
      "Water Level: 101.87\n",
      "Water Level: 101.77\n",
      "Water Level: 101.87\n",
      "Water Level: 101.77\n",
      "Water Level: 101.67\n",
      "Water Level: 101.57\n",
      "Water Level: 101.67\n",
      "Water Level: 101.57\n",
      "Water Level: 101.47\n",
      "Water Level: 101.37\n",
      "Water Level: 101.47\n",
      "Water Level: 101.37\n",
      "Water Level: 101.47\n",
      "Water Level: 101.37\n",
      "Water Level: 101.47\n",
      "Water Level: 101.37\n",
      "Water Level: 101.27\n",
      "Water Level: 101.37\n",
      "Water Level: 101.27\n",
      "Water Level: 101.17\n",
      "Water Level: 101.07\n",
      "Water Level: 100.97\n",
      "Water Level: 100.87\n",
      "Water Level: 100.77\n",
      "Water Level: 100.67\n",
      "Water Level: 100.57\n",
      "Water Level: 100.67\n",
      "Water Level: 100.57\n",
      "Water Level: 100.47\n",
      "Water Level: 100.37\n",
      "Water Level: 100.27\n",
      "Water Level: 100.17\n",
      "Water Level: 100.07\n",
      "Water Level: 99.97\n",
      "Water Level: 99.87\n",
      "Water Level: 99.97\n",
      "Water Level: 99.87\n",
      "Water Level: 99.77\n",
      "Water Level: 99.67\n",
      "Water Level: 99.57\n",
      "Water Level: 99.47\n",
      "Water Level: 99.37\n",
      "Water Level: 99.27\n",
      "Water Level: 99.17\n",
      "Water Level: 99.07\n",
      "Water Level: 98.97\n",
      "Water Level: 98.87\n",
      "Water Level: 98.77\n",
      "Water Level: 98.67\n",
      "Water Level: 98.57\n",
      "Water Level: 98.47\n",
      "Water Level: 98.37\n",
      "Water Level: 98.27\n",
      "Water Level: 98.17\n",
      "Water Level: 98.07\n",
      "Water Level: 97.97\n",
      "Water Level: 97.87\n",
      "Water Level: 97.77\n",
      "Water Level: 97.67\n",
      "Water Level: 97.57\n",
      "Water Level: 97.47\n",
      "Water Level: 97.37\n",
      "Water Level: 97.27\n",
      "Water Level: 97.17\n",
      "Water Level: 97.07\n",
      "Water Level: 96.97\n",
      "Water Level: 96.87\n",
      "Water Level: 96.77\n",
      "Water Level: 96.67\n",
      "Water Level: 96.57\n",
      "Water Level: 96.67\n",
      "Water Level: 96.57\n",
      "Water Level: 96.47\n",
      "Water Level: 96.37\n",
      "Water Level: 96.27\n",
      "Water Level: 96.37\n",
      "Water Level: 96.27\n",
      "Water Level: 96.17\n",
      "Water Level: 96.07\n",
      "Water Level: 95.97\n",
      "Water Level: 95.87\n",
      "Water Level: 95.77\n",
      "Water Level: 95.67\n",
      "Water Level: 95.57\n",
      "Water Level: 95.47\n",
      "Water Level: 95.37\n",
      "Water Level: 95.27\n",
      "Water Level: 95.17\n",
      "Water Level: 95.27\n",
      "Water Level: 95.17\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "class DamEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DamEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(2)  # 0: Close dam, 1: Open dam\n",
    "        self.observation_space = spaces.Box(low=96, high=104, shape=(1,), dtype=np.float32)\n",
    "        self.max_consecutive_steps = 100\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.seed(seed)\n",
    "        self.water_level = self.np_random.uniform(96, 104)\n",
    "        self.consecutive_steps_in_range = 0\n",
    "        return np.array([self.water_level], dtype=np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.water_level += 0.1  # Smaller increment\n",
    "        else:\n",
    "            self.water_level -= 0.1  # Smaller decrement\n",
    "        \n",
    "        in_range = 98 <= self.water_level <= 102\n",
    "\n",
    "        if in_range:\n",
    "            self.consecutive_steps_in_range += 1\n",
    "            reward = 1  # Small positive reward for being in range\n",
    "        else:\n",
    "            self.consecutive_steps_in_range = 0\n",
    "            reward = -1  # Negative reward for being out of range\n",
    "\n",
    "        terminated = self.consecutive_steps_in_range >= self.max_consecutive_steps\n",
    "        truncated = self.water_level < 90 or self.water_level > 110\n",
    "\n",
    "        return np.array([self.water_level], dtype=np.float32), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Water Level: {self.water_level:.2f}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "env = DamEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Define the model\n",
    "model = PPO(\n",
    "    'MlpPolicy', \n",
    "    env, \n",
    "    #learning_rate=0.0001,  # Smaller learning rate\n",
    "    #batch_size=32,  # Adjusted batch size\n",
    "    #gamma=0.99,  # Discount factor\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dam_dqn\")\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(\"dam_dqn\")\n",
    "\n",
    "# Test the trained model\n",
    "obs, info = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    # print(\"action\", action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561afa63",
   "metadata": {},
   "source": [
    "# 6) Simple rendering\n",
    "\n",
    "In this section we will  modify the render function to show the water level after each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16302d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Use TkAgg backend for separate windows\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DamEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DamEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(2)  # 0: Close dam, 1: Open dam\n",
    "        self.observation_space = spaces.Box(low=96, high=104, shape=(1,), dtype=np.float32)\n",
    "        self.max_consecutive_steps = 50\n",
    "        self.reset()\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.seed(seed)\n",
    "        self.water_level = self.np_random.uniform(99, 101)\n",
    "        self.consecutive_steps_in_range = 0\n",
    "        return np.array([self.water_level], dtype=np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.water_level += 0.1  # Smaller increment\n",
    "        else:\n",
    "            self.water_level -= 0.1  # Smaller decrement\n",
    "        \n",
    "        in_range = 98 <= self.water_level <= 102\n",
    "\n",
    "        if in_range:\n",
    "            self.consecutive_steps_in_range += 1\n",
    "            reward = 1  # Small positive reward for being in range\n",
    "        else:\n",
    "            self.consecutive_steps_in_range = 0\n",
    "            reward = -1  # Negative reward for being out of range\n",
    "\n",
    "        terminated = self.consecutive_steps_in_range >= self.max_consecutive_steps\n",
    "        truncated = self.water_level < 90 or self.water_level > 110\n",
    "\n",
    "        return np.array([self.water_level], dtype=np.float32), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.ax.clear()\n",
    "        \n",
    "        # Set the range for the water level\n",
    "        self.ax.set_ylim(90, 110)\n",
    "        self.ax.set_xlim(0, 1)\n",
    "        \n",
    "        # Draw the vertical bar for the water level range\n",
    "        self.ax.bar(0.5, height=110-90, width=0.1, bottom=90, color='lightblue', edgecolor='blue')\n",
    "\n",
    "        # Draw the point representing the current water level\n",
    "        self.ax.plot(0.5, self.water_level, 'ro')  # 'ro' for red dot\n",
    "        plt.axhline(y=98, color='r', linestyle='--', label=f'y = {98}')\n",
    "        plt.axhline(y=102, color='r', linestyle='--', label=f'y = {102}')\n",
    "\n",
    "    \n",
    "        # Set labels\n",
    "        self.ax.set_ylabel('Water Level (meters)')\n",
    "        self.ax.set_xticks([])\n",
    "        self.ax.set_title(f'Current Water Level: {self.water_level:.2f} meters')\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "env = DamEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(\"dam_dqn_best\")\n",
    "\n",
    "# Test the trained model\n",
    "obs, info = env.reset()\n",
    "for i in range(200):\n",
    "    action, _states = model.predict(obs)\n",
    "    # print(\"action\", action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7656a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
