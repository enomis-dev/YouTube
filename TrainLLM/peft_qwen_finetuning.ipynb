{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53441d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/.pyenv/versions/vlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 290/290 [00:00<00:00, 381.24it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b006c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(instruction, input_text=\"\"):\n",
    "    return f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_prompt(\n",
    "    \"Generate a list business ideas\",\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba7f6ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Generate a list business ideas\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. What is the best way to generate a list of business ideas?\n",
      "2. How can I get started on generating a list of business ideas?\n",
      "\n",
      "### Output:\n",
      "\n",
      "1. The best way to generate a list of business ideas is by brainstorming and identifying potential opportunities for growth.\n",
      "2. To get started on generating a list of business ideas, you can start by researching your industry or niche and identifying areas where there are gaps in the market or need for new products or services.\n",
      "3. You can also consider networking with other professionals in your field and seeking out feedback from previous clients or customers.\n",
      "4. Once you have identified potential opportunities, you can begin developing a plan for how to implement them and measure their success.\n",
      "\n",
      "Can you please provide some examples of successful business ideas that have been generated using this method?\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353182e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:5000]\")\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7163e89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a63656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "tokenized_dataset  = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9e3af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67387723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]      \n",
    "eval_dataset = train_val_split[\"test\"]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f44ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                      \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46f4df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=400,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=400,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=eval_dataset, \n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87a52423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 21:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.616290</td>\n",
       "      <td>1.541345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.605673</td>\n",
       "      <td>1.501897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.562630</td>\n",
       "      <td>1.470654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.547140</td>\n",
       "      <td>1.444177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.454252</td>\n",
       "      <td>1.429739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.492899</td>\n",
       "      <td>1.426261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=1.5268826614379882, metrics={'train_runtime': 1291.9932, 'train_samples_per_second': 7.74, 'train_steps_per_second': 1.935, 'total_flos': 1.101123944448e+16, 'train_loss': 1.5268826614379882, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7394a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 290/290 [00:01<00:00, 217.83it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"qwen/checkpoint-2500\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e831b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(instruction, input_text=\"\"):\n",
    "    return f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = build_prompt(\n",
    "    \"Generate a list of business ideas\",\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57f50293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Generate a list of business ideas\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. Start an online tutoring service\n",
      "2. Create a mobile app for businesses to manage their customer relationships \n",
      "3. Develop a virtual reality experience for employees \n",
      "4. Create a marketplace for sustainable products \n",
      "5. Launch a social media campaign for local events \n",
      "6. Offer a subscription-based service for health and wellness \n",
      "7. Create a website for a local restaurant \n",
      "8. Develop a mobile app for a fitness studio \n",
      "9. Create a blog for a startup company \n",
      "10. Launch a podcast for entrepreneurs. \n",
      "11. Create a website for a gym \n",
      "12. Develop a mobile app for a pet store \n",
      "13. Launch a website for a clothing line \n",
      "14. Create a website for a fashion brand \n",
      "15. Develop a website for a travel agency \n",
      "16. Launch a website for a tech startup \n",
      "17. Create a website for a food truck \n",
      "18. Develop a website for a yoga studio \n",
      "19. Create a website for\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
