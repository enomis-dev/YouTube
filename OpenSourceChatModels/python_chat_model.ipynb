{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a64d32",
   "metadata": {},
   "source": [
    "# LangChain and Ollama starter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb819d",
   "metadata": {},
   "source": [
    "In this notebook you will see how to use langchain ollama module to instruct a chatmodel.\n",
    "Before running this notebook you will need to have ollama installed locally for your operating system as well as the python langchain package.\n",
    "Ref: https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "\n",
    "Ollama is an advanced AI tool that allows users to easily set up and run large language models locally (in CPU and GPU modes). With Ollama, users can leverage powerful language models such as Llama 3.2 and even customize and create their own models.\n",
    "\n",
    "The models then run entirely offline allowing users to preserve their information with no costs contrary to Cloud based chat model that usually need an API_TOEKN and charge according to the usage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511e0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "# pip install -qU langchain-ollama\n",
    "# pip install -U ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9083a61",
   "metadata": {},
   "source": [
    "We can use Ollama with langchain thanks to the ChatOllama class. Here we will select one of the models that we have downlaoded beforehand.\n",
    "\n",
    "The temperature parameter in the context of language models (including ChatOllama) controls the randomness or creativity of the model's responses. It adjusts the probability distribution used during text generation\n",
    "\n",
    "A low temperature (close to 0) makes the model more deterministic and focused, generating the most likely or \"safe\" responses. This is ideal for tasks requiring accuracy, like code generation or factual summaries.\n",
    "\n",
    "A high temperature (e.g., 0.7 or higher) introduces more randomness, allowing the model to explore less likely but potentially more creative or diverse responses. This is better for creative writing or brainstorming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bfed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab69b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.2', temperature=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d0d0a",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abadf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je pense que Paris est l\\'une des plus belles villes au monde.\\n\\n(Note: I translated \"city\" to \"ville\", as it\\'s a more common and idiomatic translation in French.)', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-01-15T02:04:25.3198526Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5802547100, 'load_duration': 34624800, 'prompt_eval_count': 55, 'prompt_eval_duration': 533000000, 'eval_count': 41, 'eval_duration': 4454000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-868bcc5a-1254-4d5d-9357-0ce679bcc8bf-0', usage_metadata={'input_tokens': 55, 'output_tokens': 41, 'total_tokens': 96})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Message \n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I think Paris is one of the most beautiful city in the world.\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab012625",
   "metadata": {},
   "source": [
    "As you see here we're printing the full output message but if you want to have just the translation you can select the content.\n",
    "It also returns some notes about the translation and metadata about the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8253d27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je pense que Paris est l\\'une des plus belles villes au monde.\\n\\n(Note: I translated \"city\" to \"ville\", as it\\'s a more common and idiomatic translation in French.)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9579ad1",
   "metadata": {},
   "source": [
    "# Using prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1952f7",
   "metadata": {},
   "source": [
    "In the next cell we're going to see how to use a langchain chat template and chain it with user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8209f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant.\n",
      "Human: User asks: What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Define the system message template (instructions for the LLM)\n",
    "system_template = \"You are a helpful assistant.\"\n",
    "system_message = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Define the human message template (question or input from the user)\n",
    "human_template = \"User asks: {question}\"\n",
    "human_message = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create the full ChatPromptTemplate using the system and human message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "# Format the prompt with a dynamic question\n",
    "question = \"What is the capital of France?\"\n",
    "formatted_prompt = chat_prompt.format(question=question)\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a025376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-01-15T00:41:27.9423882Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4423927600, 'load_duration': 2794095800, 'prompt_eval_count': 45, 'prompt_eval_duration': 1016000000, 'eval_count': 8, 'eval_duration': 612000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-1be2968a-bbb9-4757-a1ba-2d0fe7976e0a-0' usage_metadata={'input_tokens': 45, 'output_tokens': 8, 'total_tokens': 53}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d08492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3e8c6",
   "metadata": {},
   "source": [
    "Key Features of ChatPromptTemplate\n",
    "\n",
    "- It allows you to design dynamic prompts where parts of the prompt are filled in with values at runtime.\n",
    "\n",
    "- Message Types: It supports different message types (e.g., system, human, assistant) to structure the conversation.\n",
    "\n",
    "- Easy Integration with LLMs: You can pass the generated prompt to an LLM (like Ollama, OpenAI or others) to get the final output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
